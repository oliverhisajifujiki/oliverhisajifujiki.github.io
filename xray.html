<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Chest X-ray Classification – Olly Fujiki</title>
  <link rel="stylesheet" href="style.css">
</head>

<body>
  <header>
    <h1>Chest X-ray Classification</h1>
    <nav>
      <a href="index.html">Home</a>
      <a href="projects.html">Projects</a>
    </nav>
  </header>

  <main>
    <section class="project-section">

      <h2>Chest X-ray Classification with ResNet</h2>

      <!-- 1️⃣ Description -->
      <details class="dropdown">
        <summary>Description</summary>
        <p>
         Over two days, I designed and trained a deep learning pipeline for medical image classification using PyTorch, comparing the performance of ResNet and DenseNet architectures. Starting with a subset of the NIH Chest X-ray dataset, I implemented custom preprocessing, balanced label generation, and GPU-accelerated training to achieve strong baseline accuracy. Along the way, I optimized training parameters, explored data scaling effects, and integrated model evaluation and visualization tools for interpretability. The result is a fully functional, extensible framework for rapid experimentation with large-scale medical imaging data.
        </p>
      </details>

      <!-- 2️⃣ Visualization -->
        <details class="dropdown" open>
        <summary>Model Demo</summary>
        <p>
            Pick any X-ray (or click “Random”) to run it through the trained network and reveal where the model focuses its attention
         </p>

        <!-- Random + Gallery -->
        <div id="gallery-section" style="text-align:center;">

            <button id="random-btn" style="margin-bottom:10px;">Pick Random X-ray</button>

            <div id="xray-gallery"
                style="display:flex; overflow-x:auto; gap:10px; padding:10px;
                        border:1px solid #ccc; border-radius:8px; max-width:800px; margin:auto;">
            </div>

            <!-- Display pair -->
            <div id="display-pair" style="margin-top:20px;">
            <img id="orig-img" src="" alt="Original X-ray"
                width="300" style="display:none; border-radius:8px; margin:10px;">
            <img id="gradcam-img" src="" alt="Grad-CAM result"
                width="300" style="display:none; border-radius:8px; margin:10px;">
            </div>

        </div>
        </details>


      <!-- 3️⃣ Model Info -->
      <details class="dropdown">
  <summary>Model Parameters</summary>

  <div class="model-info">

    <p>
      The project explores convolutional neural network (CNN) architectures for
      binary medical image classification (<em>Normal vs. Abnormal</em>).
      Models were implemented and fine-tuned using PyTorch, leveraging
      transfer learning from pretrained ImageNet weights.
    </p>

    <p>
      The baseline model, <strong>ResNet18</strong>, consists of 18 layers with
      residual skip connections that preserve gradient flow in deep networks.
      Input X-ray images were resized to 224×224, normalized to ImageNet means
      and standard deviations, and trained using a batch size of 16. Optimization
      was performed using the Adam optimizer (learning rate 1×10<sup>−4</sup>,
      weight decay 1×10<sup>−4</sup>), on a CUDA-enabled GPU for 12–15 epochs,
      employing mixed-precision training for improved efficiency.
    </p>

    <p>
      Subsequent experiments scaled up both dataset size and network complexity:
    </p>

    <ul>
      <li><strong>ResNet18:</strong> trained on 1,000, 2,500, and 25,000 labeled X-ray images.</li>
      <li><strong>ResNet50:</strong> a deeper residual network with 50 layers.</li>
      <li><strong>DenseNet121:</strong> a densely connected architecture where each layer receives input from all previous layers, promoting feature reuse and improved gradient flow.</li>
    </ul>

    <p>
      All models were evaluated on separate validation sets using overall accuracy and
      visual interpretability (<em>via Grad-CAM</em>) as primary benchmarks. The accuracy of all the models is displayed below.
    </p>
  </div>
  <figure style="text-align:center; margin-top:1rem;">
  <img src="images/NIHChestXRayAccuracyBarGraphToyExample.png"
       alt="Accuracy comparison across CNN architectures"
       width="500"
       style="border-radius:8px; border:2px solid rgba(231,98,25,0.4);">
  <figcaption style="margin-top:0.4rem; color:#562717; font-size:0.9rem;">
    Figure — Accuracy comparison across dataset sizes and CNN architectures.
  </figcaption>
</figure>

</details>

<details class="dropdown">
  <summary>Challenges and Future Work</summary>

  <div class="model-info">

    <p>
      Several key challenges emerged during training and evaluation. Early models plateaued around
      68–70% validation accuracy despite increasing dataset size, suggesting that architecture and
      hyperparameters—not just data volume—were limiting performance. Experiments revealed that
      overly high learning rates (e.g., 1×10<sup>−3</sup>) caused unstable training, while very low
      rates slowed convergence.
    </p>

    <p>
      Data quality and balance also presented issues. Many images were near-duplicates or had
      inconsistent labeling (<em>“No Finding”</em> vs. multi-label pathologies), limiting model
      generalization. Processing large tar archives was time-intensive, and GPU memory constrained
      batch sizes and training throughput.
    </p>

    <h4>Future Goals</h4>
    <ul>
      <li>Incorporating <strong>multi-class classification</strong> (identifying specific pathologies).</li>
      <li>Using <strong>semi-supervised or self-supervised pretraining</strong> to leverage unlabeled data.</li>
      <li>Testing <strong>attention-based architectures</strong> (e.g., Vision Transformers).</li>
      <li>Implementing <strong>domain adaptation</strong> to generalize across modalities (CT, MRI, Ultrasound).</li>
      <li>Improving <strong>explainability</strong> via better visualization of activation maps and decision regions.</li>
    </ul>

    <p>
      Overall, while the models achieved consistent moderate accuracy, they established a scalable,
      interpretable baseline for further exploration.
    </p>
  </div>
</details>



    </section>
  </main>


  <!-- JS for image + modal -->
  <script>

    //All the xrays either scrolled or random! 
        const totalImages = 100;
        const gallery = document.getElementById("xray-gallery");
        const origImg = document.getElementById("orig-img");
        const gradcamImg = document.getElementById("gradcam-img");
        const randomBtn = document.getElementById("random-btn");

        // build the scrollable gallery (thumbnails)
        for (let i = 1; i <= totalImages; i++) {
        const file = i.toString().padStart(3, "0");
        const thumb = document.createElement("img");
        thumb.src = `images/gradcamPairs/xray_${file}_orig.png`;
        thumb.alt = `X-ray ${file}`;
        thumb.width = 100;
        thumb.style.cursor = "pointer";
        thumb.addEventListener("click", () => showPair(file));
        gallery.appendChild(thumb);
        }

        // show random pair
        randomBtn.addEventListener("click", () => {
        const n = Math.floor(Math.random() * totalImages) + 1;
        const file = n.toString().padStart(3, "0");
        showPair(file);
        });

        // display both images
        function showPair(file) {
        origImg.src = `images/gradcamPairs/xray_${file}_orig.png`;
        gradcamImg.src = `images/gradcamPairs/xray_${file}_gradcam.png`;
        origImg.style.display = "inline-block";
        gradcamImg.style.display = "inline-block";
        }
</script>

</body>
</html>
